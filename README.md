# DNA-LM: Interpretable Transformer for TFBS Prediction
## ğŸ¯ Project Overview

**Full Title**: DNA-LM: An Interpretable Language Model for Transcription Factor Binding Site Prediction Using Self-Attention Mechanisms

**One-Line Summary**: Application of transformer-based NLP techniques to genomic sequences for interpretable prediction of transcription factor binding sites with 97.8% accuracy.

**Courses Integrated**:
- ğŸ§¬ **Bioinformatics**: ChIP-seq data analysis, regulatory genomics, motif discovery
- ğŸ¤– **Advanced Machine Learning**: Transformer architecture, attention mechanisms, deep learning optimization
- ğŸ“ **Language Processing Technologies**: DNA tokenization, sequence modeling, NLP paradigms for genomics

**Authors**: Klejda Rrapaj

**Date**: January 2026

---

## ğŸ“ Complete Project Structure

```
DNA-LM-TFBS-Prediction/
â”‚
â”œâ”€â”€ Core Implementation Files
â”‚   â”œâ”€â”€ main.py                          # Main training pipeline
â”‚   â”œâ”€â”€ DNAVocabulary.py                 # K-mer tokenization (4099 tokens)
â”‚   â”œâ”€â”€ TFBSDataset.py                   # PyTorch Dataset with data augmentation
â”‚   â”œâ”€â”€ TransformerModel.py              # Standard transformer architecture
â”‚   â”œâ”€â”€ Trainer.py                       # Training loop with early stopping
â”‚   â”œâ”€â”€ AttentionVisualizer.py           # Interpretability visualizations
â”‚   â”œâ”€â”€ PositionalEncoding.py            # Sinusoidal position encoding
â”‚   â””â”€â”€ encode_data_loader.py            # ENCODE ChIP-seq data loader
```

### Model Architecture
```
DNA Sequence (200 bp)
    â†“
Embedding Layer (4099 â†’ 128 dims)
    â†“
Positional Encoding (sinusoidal)
    â†“
Transformer Encoder Ã—4
  â€¢ Multi-head attention (8 heads)
  â€¢ Feed-forward networks
  â€¢ Layer normalization
    â†“
Classification Head (CLS token)
    â†“
Prediction: Binding (1) or Non-binding (0)

Total Parameters: 1,326,081
```

## âš™ï¸ Technical Specifications

### Hyperparameters
```python
# Model
d_model = 128           # Embedding dimension
nhead = 8              # Number of attention heads
num_layers = 4         # Transformer layers
dim_feedforward = 512  # FFN dimension
dropout = 0.1          # Dropout rate

# Training
learning_rate = 1e-4   # Adam optimizer
batch_size = 32        # Training batch size
max_epochs = 20        # Maximum epochs
early_stopping = 5     # Patience for early stopping

# Data
k = 6                  # K-mer size
max_seq_length = 200   # Sequence length
```

**Authors**: Klejda Rrapaj, Sildi Ricku

