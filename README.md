# DNA-LM: Interpretable Transformer for TFBS Prediction
## ğŸ¯ Project Overview

**Full Title**: DNA-LM: An Interpretable Language Model for Transcription Factor Binding Site Prediction Using Self-Attention Mechanisms

**One-Line Summary**: Application of transformer-based NLP techniques to genomic sequences for interpretable prediction of transcription factor binding sites with 97.8% accuracy.

**Courses Integrated**:
- ğŸ§¬ **Bioinformatics**: ChIP-seq data analysis, regulatory genomics, motif discovery
- ğŸ¤– **Advanced Machine Learning**: Transformer architecture, attention mechanisms, deep learning optimization
- ğŸ“ **Language Processing Technologies**: DNA tokenization, sequence modeling, NLP paradigms for genomics

**Authors**: Klejda Rrapaj

**Date**: January 2026

---

## ğŸ“ Complete Project Structure

```
Interpretable-LM-for-TFBS-Prediction/
â”‚
â”œâ”€â”€ models/                          # Model architectures
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ TransformerModel.py         â† Main model
â”‚   â””â”€â”€ PositionalEncoding.py       â† Model component
â”‚
â”œâ”€â”€ data/                            # Data processing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ DNAVocabulary.py            â† Tokenization
â”‚   â”œâ”€â”€ TFBSDataset.py              â† PyTorch Dataset
â”‚   â””â”€â”€ encode_data_loader.py       â† Data loading
â”‚
â”œâ”€â”€ training/                        # Training logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ Trainer.py                  â† Training loop
â”‚
â”œâ”€â”€ visualization/                   # Analysis & visualization
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ AttentionVisualizer.py      â† Attention plots
â”‚
â”œâ”€â”€ data_files/                      # Raw data
â”‚   â””â”€â”€ ENCFF308JDD.bed             â† ENCODE data
â”‚
â”œâ”€â”€ outputs/                         # Results
â”‚   â”œâ”€â”€ best_model.pt
â”‚   â””â”€â”€ attention_*.png
â”‚
â”œâ”€â”€ main.py                          # Main script
â””â”€â”€ README.md                        # Documentation
```

### Model Architecture
```
DNA Sequence (200 bp)
    â†“
Embedding Layer (4099 â†’ 128 dims)
    â†“
Positional Encoding (sinusoidal)
    â†“
Transformer Encoder Ã—4
  â€¢ Multi-head attention (8 heads)
  â€¢ Feed-forward networks
  â€¢ Layer normalization
    â†“
Classification Head (CLS token)
    â†“
Prediction: Binding (1) or Non-binding (0)

Total Parameters: 1,326,081
```

## âš™ï¸ Technical Specifications

### Hyperparameters
```python
# Model
d_model = 128           # Embedding dimension
nhead = 8              # Number of attention heads
num_layers = 4         # Transformer layers
dim_feedforward = 512  # FFN dimension
dropout = 0.1          # Dropout rate

# Training
learning_rate = 1e-4   # Adam optimizer
batch_size = 32        # Training batch size
max_epochs = 20        # Maximum epochs
early_stopping = 5     # Patience for early stopping

# Data
k = 6                  # K-mer size
max_seq_length = 200   # Sequence length
```

**Authors**: Klejda Rrapaj, Sildi Ricku

